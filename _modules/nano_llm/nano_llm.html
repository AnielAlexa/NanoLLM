<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>nano_llm.nano_llm &mdash; NanoLLM 24.7 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=b097aa5a" />

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../_static/documentation_options.js?v=66a9afe1"></script>
        <script src="../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            NanoLLM
          </a>
              <div class="version">
                24.7
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Documentation:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chat.html">Chat</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../multimodal.html">Multimodal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plugins.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../agents.html">Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../webserver.html">Webserver</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../utilities.html">Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">NanoLLM</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Module code</a></li>
      <li class="breadcrumb-item active">nano_llm.nano_llm</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for nano_llm.nano_llm</h1><div class="highlight"><pre>
<span></span><span class="ch">#!/usr/bin/env python3</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">shutil</span>
<span class="kn">import</span> <span class="nn">logging</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="kn">from</span> <span class="nn">.vision</span> <span class="kn">import</span> <span class="n">CLIPVisionModel</span><span class="p">,</span> <span class="n">MMProjector</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">AttributeDict</span><span class="p">,</span> <span class="n">convert_tensor</span><span class="p">,</span> <span class="n">download_model</span><span class="p">,</span> <span class="n">default_model_api</span><span class="p">,</span> <span class="n">print_table</span>


<div class="viewcode-block" id="NanoLLM">
<a class="viewcode-back" href="../../models.html#nano_llm.NanoLLM">[docs]</a>
<span class="k">class</span> <span class="nc">NanoLLM</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    LLM interface that model APIs implement, including:</span>
<span class="sd">    </span>
<span class="sd">      * :func:`generate` for token generation</span>
<span class="sd">      * :func:`tokenize` and :func:`detokenize`</span>
<span class="sd">      * :func:`embed_text`, :func:`embed_tokens`, and :func:`embed_image`</span>
<span class="sd">      </span>
<span class="sd">    The static method :func:`from_pretrained` will load the model using the specified API.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ModelCache</span><span class="o">=</span><span class="p">{}</span>
    
<div class="viewcode-block" id="NanoLLM.from_pretrained">
<a class="viewcode-back" href="../../models.html#nano_llm.NanoLLM.from_pretrained">[docs]</a>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">api</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a model from the given path or download it from HuggingFace Hub.</span>
<span class="sd">        Various inference and quantization APIs are supported, such as MLC and AWQ.</span>
<span class="sd">        If the API isn&#39;t explicitly specified, it will be inferred from the type of model.</span>
<span class="sd">        </span>
<span class="sd">        Base class for local LLM APIs. It defines common Huggingface-like interfaces for</span>
<span class="sd">        model loading, text generation, tokenization, embeddings, and streaming.</span>
<span class="sd">        It also supports multimodal vision models like Llava and generating image embeddings with CLIP.</span>
<span class="sd">    </span>
<span class="sd">        Args:</span>
<span class="sd">          model (str): either the path to the model, or HuggingFace model repo/name.</span>
<span class="sd">          api (str): the model backend API to use:  &#39;auto_gptq&#39;, &#39;awq&#39;, &#39;mlc&#39;, or &#39;hf&#39;</span>
<span class="sd">                       if left as None, it will attempt to be automatically determined.</span>

<span class="sd">          quantization (str): for AWQ or MLC, either specify the quantization method,</span>
<span class="sd">                              or the path to the quantized model (AWQ and MLC API&#39;s only)</span>

<span class="sd">          vision_model (str): for VLMs, override the vision embedding model </span>
<span class="sd">                              (typically `openai/clip-vit-large-patch14-336 &lt;https://huggingface.co/openai/clip-vit-large-patch14-336&gt;`_).</span>
<span class="sd">                              Otherwise, it will use the CLIP variant from the config.</span>
<span class="sd">                                </span>
<span class="sd">        Returns:</span>
<span class="sd">          A loaded `NanoLLM` model instance using the determined API.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">model_config</span> <span class="o">=</span> <span class="nb">frozenset</span><span class="p">({</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="n">model</span><span class="p">,</span> <span class="s1">&#39;api&#39;</span><span class="p">:</span> <span class="n">api</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">}</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
            <span class="n">cached_model</span> <span class="o">=</span> <span class="n">NanoLLM</span><span class="o">.</span><span class="n">ModelCache</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">model_config</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">cached_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">cached_model</span>
                
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
            <span class="n">model_path</span> <span class="o">=</span> <span class="n">model</span>
            <span class="n">model_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model_path</span> <span class="o">=</span> <span class="n">download_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">model_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            
        <span class="k">if</span> <span class="ow">not</span> <span class="n">api</span><span class="p">:</span>
            <span class="n">api</span> <span class="o">=</span> <span class="n">default_model_api</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;quantization&#39;</span><span class="p">))</span>
        
        <span class="n">api</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        
        <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_name</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;api&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">api</span>
        
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;loading </span><span class="si">{</span><span class="n">model_path</span><span class="si">}</span><span class="s2"> with </span><span class="si">{</span><span class="n">api</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">load_begin</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
        
        <span class="c1"># doing this imports here avoid circular import, and makes it so these</span>
        <span class="c1"># dependencies are only needed if they are actually used to load a model</span>
        <span class="k">if</span> <span class="n">api</span> <span class="o">==</span> <span class="s1">&#39;auto_gptq&#39;</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">nano_llm.models</span> <span class="kn">import</span> <span class="n">AutoGPTQModel</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">AutoGPTQModel</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">api</span> <span class="o">==</span> <span class="s1">&#39;awq&#39;</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">nano_llm.models</span> <span class="kn">import</span> <span class="n">AWQModel</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">AWQModel</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">api</span> <span class="o">==</span> <span class="s1">&#39;mlc&#39;</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">nano_llm.models</span> <span class="kn">import</span> <span class="n">MLCModel</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">MLCModel</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">api</span> <span class="o">==</span> <span class="s1">&#39;hf&#39;</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">nano_llm.models</span> <span class="kn">import</span> <span class="n">HFModel</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">HFModel</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;invalid API: </span><span class="si">{</span><span class="n">api</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># moved CLIP to after LLM is loaded because of MLC CUDA errors when running in subprocess</span>
        <span class="n">model</span><span class="o">.</span><span class="n">init_vision</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  
        <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">load_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">load_begin</span>
        
        <span class="n">print_table</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">NanoLLM</span><span class="o">.</span><span class="n">ModelCache</span><span class="p">[</span><span class="n">model_config</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span>
            
        <span class="k">return</span> <span class="n">model</span></div>

     
<div class="viewcode-block" id="NanoLLM.generate">
<a class="viewcode-back" href="../../models.html#nano_llm.NanoLLM.generate">[docs]</a>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">streaming</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate output from input text, tokens, or an embedding.</span>
<span class="sd">        For detailed kwarg descriptions, see `transformers.GenerationConfig &lt;https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig&gt;`_.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">        </span>
<span class="sd">          inputs (str|ndarray): Text or embedding inputs to the model/</span>
<span class="sd">          </span>
<span class="sd">          streaming (bool): If True, an iterator will be returned that returns text chunks.</span>
<span class="sd">                            Otherwise, this function will block and return the generated text.</span>
<span class="sd">                              </span>
<span class="sd">          functions(list[callable]): Dynamic functions or plugins to run inline with token generation </span>
<span class="sd">                                     for things like function calling, guidance, token healing, ect.</span>
<span class="sd">                                     These will be passed the text generated by the LLM so far, and any</span>
<span class="sd">                                     additional text that these return will be added to the chat.</span>

<span class="sd">          max_new_tokens (int): The number of tokens to output in addition to the prompt (default: 128)</span>
<span class="sd">          min_new_tokens (int): Force the model to generate a set number of output tokens (default: -1)</span>
<span class="sd">          do_sample (bool): If ``True``, temperature/top_p will be used.  Otherwise, greedy search (default: ``False``)</span>
<span class="sd">          repetition_penalty: The parameter for repetition penalty. 1.0 means no penalty (default: 1.0)</span>
<span class="sd">          temperature (float): Randomness token sampling parameter (default=0.7, only used if ``do_sample=True``)</span>
<span class="sd">          top_p (float): If set to float &lt; 1 and ``do_sample=True``, only the smallest set of most probable tokens.</span>
<span class="sd">                           with probabilities that add up to top_p or higher are kept for generation (default 0.95)</span>
<span class="sd">          stop_tokens (list[int]|list[str]): Stop generation if the bot produces tokens or text from this list (defaults to EOS token ID)</span>
<span class="sd">          kv_cache (np.ndarray): Previous kv_cache that the inputs will be appended to.  By default, a blank kv_cache </span>
<span class="sd">                                will be created for each generation (i.e. a new chat).  This generation&#39;s kv_cache</span>
<span class="sd">                                will be set in the returned :class:`StreamingResponse` iterator after the request is complete.</span>

<span class="sd">        Returns:</span>
<span class="sd">          An asynchronous :class:`StreamingResponse` iterator (when ``streaming=True``) that outputs one decoded token string at a time.</span>
<span class="sd">          Otherwise, this function blocks and a string containing the full reply is returned after it&#39;s been completed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;use LLM.from_pretrained() as opposed to instantiating an LLM object directly&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="NanoLLM.tokenize">
<a class="viewcode-back" href="../../models.html#nano_llm.NanoLLM.tokenize">[docs]</a>
    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;np&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Tokenize the given string and return the encoded token ID&#39;s.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">          text (str): the text to tokenize.</span>
<span class="sd">          add_special_tokens (str): if BOS/EOS tokens (like ``&lt;s&gt;`` or ``&lt;|endoftext|&gt;``) should automatically be added (default False)</span>
<span class="sd">          dtype (type): the numpy or torch datatype of the tensor to return.</span>
<span class="sd">          return_tensors (str): ``&#39;np&#39;`` to return a `np.ndarray` or ``&#39;pt&#39;`` to return a `torch.Tensor`</span>
<span class="sd">          kwargs:  additional arguments forwarded to the HuggingFace `transformers.AutoTokenizer &lt;https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer&gt;`_ encode function.</span>
<span class="sd">          </span>
<span class="sd">        Returns:</span>
<span class="sd">          The token ID&#39;s with the tensor type as indicated by `return_tensors` (either `&#39;np&#39;` for `np.ndarray`</span>
<span class="sd">          or `&#39;pt&#39;` for `torch.Tensor`) and datatype as indicated by `dtype` (by default ``int32``)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">return_tensors</span> <span class="o">==</span> <span class="s1">&#39;tvm&#39;</span><span class="p">:</span>
            <span class="n">return_tensors</span> <span class="o">=</span> <span class="s1">&#39;np&#39;</span>
  
        <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span>
            <span class="n">text</span><span class="p">,</span> 
            <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span><span class="p">,</span> 
            <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>

        <span class="k">return</span> <span class="n">convert_tensor</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="NanoLLM.detokenize">
<a class="viewcode-back" href="../../models.html#nano_llm.NanoLLM.detokenize">[docs]</a>
    <span class="k">def</span> <span class="nf">detokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Detokenize the given token ID&#39;s and return the decoded string.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">          tokens (list[int], np.ndarray, torch.Tensor): the array of token ID&#39;s</span>
<span class="sd">          skip_special_tokens (bool): if special tokens (like BOS/EOS) should be supressed from the output or not (default false)</span>
<span class="sd">          kwargs:  additional arguments forwarded to the HuggingFace `transformers.AutoTokenizer &lt;https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer&gt;`_ decode function.</span>
<span class="sd">          </span>
<span class="sd">        Returns:</span>
<span class="sd">          The string containing the decoded text.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="n">skip_special_tokens</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

        
<div class="viewcode-block" id="NanoLLM.embed_text">
<a class="viewcode-back" href="../../models.html#nano_llm.NanoLLM.embed_text">[docs]</a>
    <span class="k">def</span> <span class="nf">embed_text</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;np&#39;</span><span class="p">,</span> <span class="n">return_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Tokenize the string with :meth:`NanoLLM.tokenize` and return its embedding as computed by :meth:`NanoLLM.embed_tokens`.</span>
<span class="sd">        Note that if ``model.has_embed=False``, then None will be returned for the embedding and the tokens should be used instead.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">          text (str): the text to tokenize and embed.</span>
<span class="sd">          add_special_tokens (str): if BOS/EOS tokens (like ``&lt;s&gt;``, ``&lt;|endoftext|&gt;``) should automatically be added (default False)</span>
<span class="sd">          use_cache (bool): if True, the text embedding will be cached and returned without additional computation if</span>
<span class="sd">                            the same string was already embedded previously.  This is useful for things like the system prompt</span>
<span class="sd">                            that are relatively static, but probably shouldn&#39;t be used for dynamic user inputs that are unlikely</span>
<span class="sd">                            to be re-used again (leading to unnecessarily increased memory usage).  The default is false.</span>
<span class="sd">          return_tensors (str): ``&#39;np&#39;`` to return a `np.ndarray` or ``&#39;pt&#39;`` to return a `torch.Tensor`</span>
<span class="sd">          return_tokens (bool): if True, then the tokens will also be returned in addition to the embedding.</span>
<span class="sd">          kwargs:  additional arguments forwarded to :meth:`NanoLLM.tokenize` and the HuggingFace `transformers.AutoTokenizer &lt;https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer&gt;`_ </span>
<span class="sd">          </span>
<span class="sd">        Returns:</span>
<span class="sd">          The embedding with the tensor type as indicated by `return_tensors` (either `&#39;np&#39;` for `np.ndarray`</span>
<span class="sd">          or `&#39;pt&#39;` for `torch.Tensor`) with ``float32`` data.  If ``return_tokens=True``, then an (embedding, tokens)</span>
<span class="sd">          tuple will be returned instead of only the embeddings. If ``model.has_embed=False`, then the embedding will be None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">result</span> <span class="o">=</span> <span class="kc">None</span>
        
        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_cache</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;text embedding cache hit `</span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s1">`&#39;</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\\</span><span class="s1">n&#39;</span><span class="p">))</span>
            
        <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">embed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_embed</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">result</span> <span class="o">=</span> <span class="p">(</span><span class="n">embed</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embed_cache</span><span class="p">[</span><span class="n">text</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span>
            
        <span class="k">if</span> <span class="n">return_tokens</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">result</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="NanoLLM.embed_tokens">
<a class="viewcode-back" href="../../models.html#nano_llm.NanoLLM.embed_tokens">[docs]</a>
    <span class="k">def</span> <span class="nf">embed_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;np&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the token embedding and return its tensor.  This will raise an exception if ``model.has_embed=False``.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">          tokens (list[int], np.ndarray, torch.Tensor): the array of token ID&#39;s</span>
<span class="sd">          return_tensors (str): ``&#39;np&#39;`` to return a `np.ndarray` or ``&#39;pt&#39;`` to return a `torch.Tensor`</span>
<span class="sd">          </span>
<span class="sd">        Returns:</span>
<span class="sd">          The embedding with the tensor type as indicated by `return_tensors` (either `&#39;np&#39;` for `np.ndarray`</span>
<span class="sd">          or `&#39;pt&#39;` for `torch.Tensor`) with ``float32`` data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;embed_tokens() not implemented for this model&quot;</span><span class="p">)</span></div>

       
<div class="viewcode-block" id="NanoLLM.embed_image">
<a class="viewcode-back" href="../../models.html#nano_llm.NanoLLM.embed_image">[docs]</a>
    <span class="k">def</span> <span class="nf">embed_image</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the embedding of an image (for multimodel models with a vision encoder like CLIP),</span>
<span class="sd">        and apply any additional projection layers as specified by the model.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">          image (pil.Image, np.ndarray, torch.Tensor, jetson.utils.cudaImage, __cuda_array_interface__): the image</span>
<span class="sd">          return_tensors (str): ``&#39;np&#39;`` to return a `np.ndarray` or ``&#39;pt&#39;`` to return a `torch.Tensor` (on the GPU)</span>
<span class="sd">          return_dict (bool): if true, return a dict including the vision encoder&#39;s `hidden_state` and `embedding`</span>
<span class="sd">          kwargs: additional arguments forwarded to the vision encoder (`nano_llm.vision.CLIPImageEmbedding`)</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">          The embedding with the tensor type as indicated by `return_tensors` (either `&#39;np&#39;` for `np.ndarray`</span>
<span class="sd">          or `&#39;pt&#39;` for `torch.Tensor`), or a dict containing the embedding and vision encoder&#39;s `hidden_state`</span>
<span class="sd">          if ``return_dict=True``.</span>
<span class="sd">        &quot;&quot;&quot;</span>  
        <span class="k">assert</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">has_vision</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">hidden_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">mm_vision_select_layer</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">)</span>
        
        <span class="n">embedding</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">hidden_state</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="k">else</span> <span class="n">output</span>
        <span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
        <span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mm_projector</span><span class="p">(</span><span class="n">embedding</span> <span class="k">if</span> <span class="s1">&#39;mm_projector_cfg&#39;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="k">else</span> <span class="n">embedding</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span>

        <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;image embedding  shape=</span><span class="si">{</span><span class="n">embedding</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">  dtype=</span><span class="si">{</span><span class="n">embedding</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">  device=</span><span class="si">{</span><span class="n">embedding</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">output</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">output</span><span class="p">:</span>
                <span class="n">output</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">convert_tensor</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">output</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">convert_tensor</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">)</span></div>

        
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_path</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1">#: HuggingFace `transformers.AutoTokenizer &lt;https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer&gt;`_ instance used for tokenization/detokenization.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1">#: Dict containing the model configuration (inspect it on the HuggingFace model card)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">AttributeDict</span><span class="p">()</span>
        
        <span class="c1">#: The local path to the model config file (``config.json``)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="s1">&#39;config.json&#39;</span><span class="p">)</span>
        
        <span class="c1">#: The local path to the model checkpoint/weights in HuggingFace format.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_path</span> <span class="o">=</span> <span class="n">model_path</span>

        <span class="c1"># load the config file</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config_path</span><span class="p">):</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config_path</span><span class="p">)</span> <span class="k">as</span> <span class="n">config_file</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">AttributeDict</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">config_file</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;could not find model config file at </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">AttributeDict</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">api</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;api&#39;</span><span class="p">)</span>
        
        <span class="c1">#: Dict containing the latest generation performance statistics.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stats</span> <span class="o">=</span> <span class="n">AttributeDict</span><span class="p">()</span>
        
        <span class="c1">#: True if this is a multimodal vision/language model.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_vision</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config_vision</span><span class="p">()</span>
        
        <span class="c1">#: True if this model has a separate text embedding layer for embed_text()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_embed</span> <span class="o">=</span> <span class="kc">False</span>
        
        <span class="c1"># token and embedding caches</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_cache</span> <span class="o">=</span> <span class="p">{}</span>
        
        <span class="c1"># create the tokenizer        </span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_path</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_path</span><span class="p">,</span> <span class="n">use_fast</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            
        
    <span class="k">def</span> <span class="nf">patch_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Update the original HF model&#39;s config.json with different settings from the provided kwargs.</span>
        <span class="c1"># The original will be saved under the same directory to &#39;config.json.backup&#39;</span>
        <span class="n">backup_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config_path</span> <span class="o">+</span> <span class="s1">&#39;.backup&#39;</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">backup_path</span><span class="p">):</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;backing up original model config to </span><span class="si">{</span><span class="n">backup_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">shutil</span><span class="o">.</span><span class="n">copyfile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config_path</span><span class="p">,</span> <span class="n">backup_path</span><span class="p">)</span>
            
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;patching model config with </span><span class="si">{</span><span class="n">kwargs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="n">patched_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="c1">#.copy()</span>
        <span class="n">patched_config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config_path</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">config_file</span><span class="p">:</span>
            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">patched_config</span><span class="p">,</span> <span class="n">config_file</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
     
    <span class="k">def</span> <span class="nf">config_vision</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Check the model config for multimodal support (can be in a variety of formats)</span>
        <span class="n">model_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="n">has_vision</span> <span class="o">=</span> <span class="s1">&#39;llava&#39;</span> <span class="ow">in</span> <span class="n">model_type</span>
        
        <span class="c1"># patch the config to change llava to llama so the quant tools handle it</span>
        <span class="k">if</span> <span class="n">has_vision</span><span class="p">:</span>
            <span class="k">if</span> <span class="s1">&#39;stablelm&#39;</span> <span class="ow">in</span> <span class="n">model_type</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">patch_config</span><span class="p">(</span><span class="n">model_type</span><span class="o">=</span><span class="s1">&#39;stablelm_epoch&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="s1">&#39;phi&#39;</span> <span class="ow">in</span> <span class="n">model_type</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">patch_config</span><span class="p">(</span><span class="n">model_type</span><span class="o">=</span><span class="s1">&#39;phi&#39;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">patch_config</span><span class="p">(</span><span class="n">model_type</span><span class="o">=</span><span class="s1">&#39;llama&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">name_or_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;_name_or_path&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">name_or_path</span><span class="p">:</span>
                <span class="n">has_vision</span> <span class="o">=</span> <span class="s1">&#39;llava&#39;</span> <span class="ow">in</span> <span class="n">name_or_path</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">arch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;architectures&#39;</span><span class="p">,</span> <span class="p">[]):</span>
            <span class="k">if</span> <span class="s1">&#39;llava&#39;</span> <span class="ow">in</span> <span class="n">arch</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">or</span> <span class="s1">&#39;bunny&#39;</span> <span class="ow">in</span> <span class="n">arch</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
                <span class="n">has_vision</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;bunny-stablelm&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">patch_config</span><span class="p">(</span><span class="n">model_type</span><span class="o">=</span><span class="s1">&#39;stablelm_epoch&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;bunny-phi&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">patch_config</span><span class="p">(</span><span class="n">model_type</span><span class="o">=</span><span class="s1">&#39;phi&#39;</span><span class="p">)</span>
            
        <span class="c1"># support checkpoints with LLM and vision encoder under separate subdirectories</span>
        <span class="k">if</span> <span class="s1">&#39;vision_tower_cfg&#39;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">:</span>
            <span class="n">vision_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_path</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;vision_tower_cfg&#39;</span><span class="p">][</span><span class="s1">&#39;_name_or_path&#39;</span><span class="p">]))</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">vision_path</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">IOError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;multimodal config was for separate models, but could not find </span><span class="si">{</span><span class="n">vision_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="s1">&#39;mm_vision_tower&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;mm_vision_tower&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vision_path</span>
        
        <span class="k">if</span> <span class="s1">&#39;mm_projector_cfg&#39;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">mm_projector_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_path</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;mm_projector_cfg&#39;</span><span class="p">][</span><span class="s1">&#39;_name_or_path&#39;</span><span class="p">]))</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">mm_projector_path</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">IOError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;multimodal config was for separate models, but could not find </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">mm_projector_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="s1">&#39;mm_projector_type&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;mm_projector_type&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;mm_projector_cfg&#39;</span><span class="p">][</span><span class="s1">&#39;mm_projector_type&#39;</span><span class="p">]</span>

        <span class="k">if</span> <span class="s1">&#39;mm_projector_path&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">mm_projector_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_path</span>
                          
        <span class="k">if</span> <span class="s1">&#39;llm_cfg&#39;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">:</span>
            <span class="n">llm_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_path</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;llm_cfg&#39;</span><span class="p">][</span><span class="s1">&#39;_name_or_path&#39;</span><span class="p">]))</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">llm_path</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">IOError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;multimodal config was for separate models, but could not find </span><span class="si">{</span><span class="n">llm_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">llm_path</span><span class="p">,</span> <span class="s1">&#39;config.json&#39;</span><span class="p">))</span> <span class="k">as</span> <span class="n">config_file</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">config_file</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_path</span> <span class="o">=</span> <span class="n">llm_path</span>  <span class="c1"># redirect downstream LLM APIs to the LLM model</span>
          
        <span class="k">return</span> <span class="n">has_vision</span>
               
    <span class="k">def</span> <span class="nf">init_vision</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vision_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vision_api</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Load the vision encoder (CLIP/SigLIP) and mm_projector for multimodal models</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_vision</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># load the image embedding model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vision</span> <span class="o">=</span> <span class="n">CLIPVisionModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
            <span class="n">vision_model</span> <span class="k">if</span> <span class="n">vision_model</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">mm_vision_tower</span><span class="p">,</span>
            <span class="n">crop</span><span class="o">=</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;vision_scaling&#39;</span><span class="p">,</span> <span class="s1">&#39;resize&#39;</span><span class="p">)</span> <span class="o">==</span> <span class="s1">&#39;crop&#39;</span><span class="p">),</span>
            <span class="n">use_tensorrt</span><span class="o">=</span><span class="p">(</span><span class="n">vision_api</span> <span class="o">==</span> <span class="s1">&#39;auto&#39;</span> <span class="ow">or</span> <span class="n">vision_api</span> <span class="o">==</span> <span class="s1">&#39;trt&#39;</span><span class="p">),</span> 
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
        <span class="p">)</span> 
        
        <span class="c1"># create image embedding projection model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mm_projector</span> <span class="o">=</span> <span class="n">MMProjector</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span></div>


</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>