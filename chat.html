<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Chat &mdash; NanoLLM 24.5.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css?v=b097aa5a" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=c34423f4"></script>
        <script src="_static/doctools.js?v=9a2dae69"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Multimodal" href="multimodal.html" />
    <link rel="prev" title="Models" href="models.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            NanoLLM
          </a>
              <div class="version">
                24.5.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Documentation:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Models</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Chat</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#code-example">Code Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="#templates">Templates</a></li>
<li class="toctree-l2"><a class="reference internal" href="#chat-history">Chat History</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#nano_llm.ChatHistory"><code class="docutils literal notranslate"><span class="pre">ChatHistory</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatHistory.kv_cache"><code class="docutils literal notranslate"><span class="pre">ChatHistory.kv_cache</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatHistory.num_tokens"><code class="docutils literal notranslate"><span class="pre">ChatHistory.num_tokens</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatHistory.__len__"><code class="docutils literal notranslate"><span class="pre">ChatHistory.__len__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatHistory.__getitem__"><code class="docutils literal notranslate"><span class="pre">ChatHistory.__getitem__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatHistory.__delitem__"><code class="docutils literal notranslate"><span class="pre">ChatHistory.__delitem__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatHistory.append"><code class="docutils literal notranslate"><span class="pre">ChatHistory.append()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatHistory.pop"><code class="docutils literal notranslate"><span class="pre">ChatHistory.pop()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatHistory.remove"><code class="docutils literal notranslate"><span class="pre">ChatHistory.remove()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatHistory.reset"><code class="docutils literal notranslate"><span class="pre">ChatHistory.reset()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatHistory.to_list"><code class="docutils literal notranslate"><span class="pre">ChatHistory.to_list()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatHistory.system_prompt"><code class="docutils literal notranslate"><span class="pre">ChatHistory.system_prompt</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatHistory.embed_chat"><code class="docutils literal notranslate"><span class="pre">ChatHistory.embed_chat()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatHistory.reindex"><code class="docutils literal notranslate"><span class="pre">ChatHistory.reindex()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatHistory.find_wrap_entry"><code class="docutils literal notranslate"><span class="pre">ChatHistory.find_wrap_entry()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#chat-message">Chat Message</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#nano_llm.ChatMessage"><code class="docutils literal notranslate"><span class="pre">ChatMessage</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatMessage.role"><code class="docutils literal notranslate"><span class="pre">ChatMessage.role</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatMessage.template"><code class="docutils literal notranslate"><span class="pre">ChatMessage.template</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatMessage.tokens"><code class="docutils literal notranslate"><span class="pre">ChatMessage.tokens</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatMessage.embedding"><code class="docutils literal notranslate"><span class="pre">ChatMessage.embedding</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatMessage.history"><code class="docutils literal notranslate"><span class="pre">ChatMessage.history</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatMessage.use_cache"><code class="docutils literal notranslate"><span class="pre">ChatMessage.use_cache</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatMessage.cached"><code class="docutils literal notranslate"><span class="pre">ChatMessage.cached</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatMessage.index"><code class="docutils literal notranslate"><span class="pre">ChatMessage.index</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatMessage.prev"><code class="docutils literal notranslate"><span class="pre">ChatMessage.prev</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatMessage.next"><code class="docutils literal notranslate"><span class="pre">ChatMessage.next</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatMessage.type"><code class="docutils literal notranslate"><span class="pre">ChatMessage.type</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatMessage.content"><code class="docutils literal notranslate"><span class="pre">ChatMessage.content</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatMessage.num_tokens"><code class="docutils literal notranslate"><span class="pre">ChatMessage.num_tokens</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatMessage.start_token"><code class="docutils literal notranslate"><span class="pre">ChatMessage.start_token</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatMessage.content_type"><code class="docutils literal notranslate"><span class="pre">ChatMessage.content_type()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatMessage.is_type"><code class="docutils literal notranslate"><span class="pre">ChatMessage.is_type()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.ChatMessage.embed"><code class="docutils literal notranslate"><span class="pre">ChatMessage.embed()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#function-calling">Function Calling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#nano_llm.bot_function"><code class="docutils literal notranslate"><span class="pre">bot_function()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#nano_llm.BotFunctions"><code class="docutils literal notranslate"><span class="pre">BotFunctions</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.BotFunctions.__new__"><code class="docutils literal notranslate"><span class="pre">BotFunctions.__new__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.BotFunctions.len"><code class="docutils literal notranslate"><span class="pre">BotFunctions.len()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.BotFunctions.list"><code class="docutils literal notranslate"><span class="pre">BotFunctions.list()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.BotFunctions.filter"><code class="docutils literal notranslate"><span class="pre">BotFunctions.filter()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.BotFunctions.generate_docs"><code class="docutils literal notranslate"><span class="pre">BotFunctions.generate_docs()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.BotFunctions.register"><code class="docutils literal notranslate"><span class="pre">BotFunctions.register()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.BotFunctions.load"><code class="docutils literal notranslate"><span class="pre">BotFunctions.load()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#nano_llm.BotFunctions.test"><code class="docutils literal notranslate"><span class="pre">BotFunctions.test()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="multimodal.html">Multimodal</a></li>
<li class="toctree-l1"><a class="reference internal" href="plugins.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="agents.html">Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="webserver.html">Webserver</a></li>
<li class="toctree-l1"><a class="reference internal" href="utilities.html">Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="releases.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NanoLLM</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Chat</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/chat.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="chat">
<h1>Chat<a class="headerlink" href="#chat" title="Link to this heading"></a></h1>
<p>This page includes information about managing multi-turn chat sessions, templating, and maintaining the embedding history.  Here’s how to run it interactively from the terminal:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>nano_llm.chat<span class="w"> </span>--api<span class="w"> </span>mlc<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model<span class="w"> </span>meta-llama/Meta-Llama-3-8B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--quantization<span class="w"> </span>q4f16_ft
</pre></div>
</div>
<p>If you load a multimodal model (like <code class="docutils literal notranslate"><span class="pre">liuhaotian/llava-v1.6-vicuna-7b</span></code>), you can enter image filenames or URLs and a query to chat about images.  Enter <code class="docutils literal notranslate"><span class="pre">/reset</span></code> to reset the chat history.</p>
<section id="code-example">
<h2>Code Example<a class="headerlink" href="#code-example" title="Link to this heading"></a></h2>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python3</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">termcolor</span>

<span class="kn">from</span> <span class="nn">nano_llm</span> <span class="kn">import</span> <span class="n">NanoLLM</span><span class="p">,</span> <span class="n">ChatHistory</span>

<span class="c1"># parse arguments</span>
<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">formatter_class</span><span class="o">=</span><span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentDefaultsHelpFormatter</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--model&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;meta-llama/Meta-Llama-3-8B-Instruct&#39;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;path to the model, or HuggingFace model repo&quot;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max-new-tokens&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the maximum response length for each bot reply&quot;</span><span class="p">)</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

<span class="c1"># load model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">NanoLLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> 
    <span class="n">quantization</span><span class="o">=</span><span class="s1">&#39;q4f16_ft&#39;</span><span class="p">,</span> 
    <span class="n">api</span><span class="o">=</span><span class="s1">&#39;mlc&#39;</span>
<span class="p">)</span>

<span class="c1"># create the chat history</span>
<span class="n">chat_history</span> <span class="o">=</span> <span class="n">ChatHistory</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">system_prompt</span><span class="o">=</span><span class="s2">&quot;You are a helpful and friendly AI assistant.&quot;</span><span class="p">)</span>

<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="c1"># enter the user query from terminal</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&gt;&gt; &#39;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="nb">input</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

    <span class="c1"># add user prompt and generate chat tokens/embeddings</span>
    <span class="n">chat_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="o">=</span><span class="n">prompt</span><span class="p">)</span>
    <span class="n">embedding</span><span class="p">,</span> <span class="n">position</span> <span class="o">=</span> <span class="n">chat_history</span><span class="o">.</span><span class="n">embed_chat</span><span class="p">()</span>

    <span class="c1"># generate bot reply</span>
    <span class="n">reply</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="n">embedding</span><span class="p">,</span> 
        <span class="n">streaming</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
        <span class="n">kv_cache</span><span class="o">=</span><span class="n">chat_history</span><span class="o">.</span><span class="n">kv_cache</span><span class="p">,</span>
        <span class="n">stop_tokens</span><span class="o">=</span><span class="n">chat_history</span><span class="o">.</span><span class="n">template</span><span class="o">.</span><span class="n">stop</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">max_new_tokens</span><span class="p">,</span>
    <span class="p">)</span>
        
    <span class="c1"># stream the output</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">reply</span><span class="p">:</span>
        <span class="n">termcolor</span><span class="o">.</span><span class="n">cprint</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span> <span class="k">if</span> <span class="n">reply</span><span class="o">.</span><span class="n">eos</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># save the final output</span>
    <span class="n">chat_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s1">&#39;bot&#39;</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="n">reply</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">tokens</span><span class="o">=</span><span class="n">reply</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>
    <span class="n">chat_history</span><span class="o">.</span><span class="n">kv_cache</span> <span class="o">=</span> <span class="n">reply</span><span class="o">.</span><span class="n">kv_cache</span>
</pre></div>
</div>
</section>
<section id="templates">
<h2>Templates<a class="headerlink" href="#templates" title="Link to this heading"></a></h2>
<p>These are the built-in chat templates that are automatically determined from the model type, or settable with the <code class="docutils literal notranslate"><span class="pre">--chat-template</span></code> command-line argument:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="o">*</span> <span class="n">llama</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">llama</span><span class="o">-</span><span class="mi">3</span>
<span class="o">*</span> <span class="n">vicuna</span><span class="o">-</span><span class="n">v0</span><span class="p">,</span> <span class="n">vicuna</span><span class="o">-</span><span class="n">v1</span>
<span class="o">*</span> <span class="n">stablelm</span><span class="o">-</span><span class="n">zephyr</span>
<span class="o">*</span> <span class="n">chat</span><span class="o">-</span><span class="n">ml</span>
<span class="o">*</span> <span class="n">sheared</span><span class="o">-</span><span class="n">llama</span>
<span class="o">*</span> <span class="n">nous</span><span class="o">-</span><span class="n">obsidian</span>
<span class="o">*</span> <span class="n">phi</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="n">chat</span><span class="p">,</span> <span class="n">phi</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="n">instruct</span>
<span class="o">*</span> <span class="n">gemma</span>
</pre></div>
</div>
<p>See <code class="docutils literal notranslate"><span class="pre">nano_llm/chat/templates.py</span></code> for them.  You can also specify a JSON file containing the template.</p>
</section>
<section id="chat-history">
<h2>Chat History<a class="headerlink" href="#chat-history" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="nano_llm.ChatHistory">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">ChatHistory</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chat_template</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">system_prompt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/chat/history.html#ChatHistory"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.ChatHistory" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Multimodal chat history that can contain a mix of media including text/images.</p>
<p>ChatHistory objects can be indexed like a list to access its messages,
where each <a class="reference internal" href="#nano_llm.ChatMessage" title="nano_llm.ChatMessage"><code class="xref py py-class docutils literal notranslate"><span class="pre">ChatMessage</span></code></a> can have a different type of content:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">chat_history</span><span class="p">[</span><span class="n">n</span><span class="p">]</span>  <span class="c1"># will return the n-th chat entry</span>
</pre></div>
</div>
<p>Each type of media has an associated embedding function (e.g. LLM’s typically
do text token embedding internally, and images use CLIP + projection layers).
From these, it assembles the embedding for the entire chat as input to the LLM.</p>
<p>It uses templating to add the required special tokens as defined by different
model architectures.  In normal 2-turn chat, there are ‘user’ and ‘bot’ roles
defined, but arbitrary roles can be added, each with their own template.</p>
<p>The system prompt can also be configured through the chat template
and by setting the <a class="reference internal" href="#nano_llm.ChatHistory.system_prompt" title="nano_llm.ChatHistory.system_prompt"><code class="xref py py-attr docutils literal notranslate"><span class="pre">ChatHistory.system_prompt</span></code></a> property.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="nano_llm.ChatHistory.kv_cache">
<span class="sig-name descname"><span class="pre">kv_cache</span></span><a class="headerlink" href="#nano_llm.ChatHistory.kv_cache" title="Link to this definition"></a></dt>
<dd><p>The <a class="reference internal" href="models.html#nano_llm.KVCache" title="nano_llm.KVCache"><code class="xref py py-class docutils literal notranslate"><span class="pre">KVCache</span></code></a> from <a class="reference internal" href="models.html#nano_llm.NanoLLM.generate" title="nano_llm.NanoLLM.generate"><code class="xref py py-meth docutils literal notranslate"><span class="pre">NanoLLM.generate()</span></code></a> used to store the model state.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nano_llm.ChatHistory.num_tokens">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">num_tokens</span></span><a class="headerlink" href="#nano_llm.ChatHistory.num_tokens" title="Link to this definition"></a></dt>
<dd><p>Return the number of tokens used by the chat so far.
<a class="reference internal" href="#nano_llm.ChatHistory.embed_chat" title="nano_llm.ChatHistory.embed_chat"><code class="xref py py-meth docutils literal notranslate"><span class="pre">embed_chat()</span></code></a> needs to have been called for this to be upated,
because otherwise the input wouldn’t have been tokenized yet.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.ChatHistory.__len__">
<span class="sig-name descname"><span class="pre">__len__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/chat/history.html#ChatHistory.__len__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.ChatHistory.__len__" title="Link to this definition"></a></dt>
<dd><p>Returns the number of messages in the chat history</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.ChatHistory.__getitem__">
<span class="sig-name descname"><span class="pre">__getitem__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">key</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/chat/history.html#ChatHistory.__getitem__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.ChatHistory.__getitem__" title="Link to this definition"></a></dt>
<dd><p>Return the n-th chat message with the subscript indexing operator</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.ChatHistory.__delitem__">
<span class="sig-name descname"><span class="pre">__delitem__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">key</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/chat/history.html#ChatHistory.__delitem__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.ChatHistory.__delitem__" title="Link to this definition"></a></dt>
<dd><p>Remove one or more messages from the chat history:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="k">del</span> <span class="n">chat_history</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>   <span class="c1"># remove the second-to-last entry</span>
<span class="k">del</span> <span class="n">chat_history</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>  <span class="c1"># pop the last 2 entries</span>
<span class="k">del</span> <span class="n">chat_history</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>   <span class="c1"># remove all entries but the first</span>
</pre></div>
</div>
<p>This will also update the KV cache and alter the bot memory.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.ChatHistory.append">
<span class="sig-name descname"><span class="pre">append</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">role</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'user'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">msg</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/chat/history.html#ChatHistory.append"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.ChatHistory.append" title="Link to this definition"></a></dt>
<dd><p>Add a chat entry consisting of a text message, image, ect.
See the <a class="reference internal" href="#nano_llm.ChatMessage" title="nano_llm.ChatMessage"><code class="xref py py-class docutils literal notranslate"><span class="pre">ChatMessage</span></code></a> class for description of arguments.
This can also accept an existing <a class="reference internal" href="#nano_llm.ChatMessage" title="nano_llm.ChatMessage"><code class="xref py py-class docutils literal notranslate"><span class="pre">ChatMessage</span></code></a> set to <code class="docutils literal notranslate"><span class="pre">msg</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.ChatHistory.pop">
<span class="sig-name descname"><span class="pre">pop</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">count</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/chat/history.html#ChatHistory.pop"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.ChatHistory.pop" title="Link to this definition"></a></dt>
<dd><p>Remove the last N messages from the chat and KV cache.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.ChatHistory.remove">
<span class="sig-name descname"><span class="pre">remove</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">start</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/chat/history.html#ChatHistory.remove"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.ChatHistory.remove" title="Link to this definition"></a></dt>
<dd><p>Remove the chat entries from the start (inclusive) to stop (exclusive) indexes.
If stop is not specified, then only the single entry at the start index will be removed:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">chat_history</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>    <span class="c1"># remove the first chat entry</span>
<span class="n">chat_history</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># remove the first and second chat entries</span>
<span class="n">chat_history</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># remove the last chat entry</span>
<span class="n">chat_history</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># remove the last two entries</span>
</pre></div>
</div>
<p>This will also update the KV cache and alter the bot’s memory (potentially destructively)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.ChatHistory.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">add_system_prompt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_cache</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/chat/history.html#ChatHistory.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.ChatHistory.reset" title="Link to this definition"></a></dt>
<dd><p>Reset the chat history, and optionally add the system prompt to the new chat.
If <code class="docutils literal notranslate"><span class="pre">use_cache=True</span></code>, then the system prompt tokens/embedding will be cached.
If <cite>wrap_tokens</cite> is set, then the most recent N tokens from the chat will be kept.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.ChatHistory.to_list">
<span class="sig-name descname"><span class="pre">to_list</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/chat/history.html#ChatHistory.to_list"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.ChatHistory.to_list" title="Link to this definition"></a></dt>
<dd><p>Serialize the history to a list of dicts, where each dict is a chat entry
with the non-critical keys removed (suitable for web transport, ect)</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nano_llm.ChatHistory.system_prompt">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">system_prompt</span></span><a class="headerlink" href="#nano_llm.ChatHistory.system_prompt" title="Link to this definition"></a></dt>
<dd><p>Get the system prompt, the typically hidden instruction at the beginning
of the chat like “You are a curious and helpful AI assistant, …”</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.ChatHistory.embed_chat">
<span class="sig-name descname"><span class="pre">embed_chat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_cache</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wrap_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/chat/history.html#ChatHistory.embed_chat"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.ChatHistory.embed_chat" title="Link to this definition"></a></dt>
<dd><p>Assemble the embedding of either the latest or entire chat.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">use_cache=True</span></code> (the default), and only the new embeddings will be returned.
If <code class="docutils literal notranslate"><span class="pre">use_cache=False</span></code>, then the entire chat history will be returned.</p>
<p>This function returns an <code class="docutils literal notranslate"><span class="pre">(embedding,</span> <span class="pre">position)</span></code> tuple, where the embedding array
contains the new embeddings (or tokens) from the chat, and position is the current
overall position in the history (up to the model’s context window length)</p>
<p>If the number of tokens in the chat history exceeds the length given in <code class="docutils literal notranslate"><span class="pre">max_tokens</span></code> argument
(which is typically the model’s context window, minus the max generation length),
then the chat history will drop all but the latest <code class="docutils literal notranslate"><span class="pre">wrap_tokens</span></code>, starting with a user prompt.
If <cite>max_tokens</cite> is provided but <cite>wrap_tokens</cite> is not, then the overflow tokens will be truncated.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.ChatHistory.reindex">
<span class="sig-name descname"><span class="pre">reindex</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/chat/history.html#ChatHistory.reindex"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.ChatHistory.reindex" title="Link to this definition"></a></dt>
<dd><p>Update the linked lists in the messages that refer to each other.
This gets called after messages are added, removed, or their order changed.
You wouldn’t typically need to call this yourself.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.ChatHistory.find_wrap_entry">
<span class="sig-name descname"><span class="pre">find_wrap_entry</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">wrap_tokens</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/chat/history.html#ChatHistory.find_wrap_entry"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.ChatHistory.find_wrap_entry" title="Link to this definition"></a></dt>
<dd><p>Find the oldest entry from which the chat doesn’t exceed the number of wrap_tokens,
and that the entry should be a user query.  This is used to keep those more recent
chat entries when the history overflows past the max context window of the model.</p>
</dd></dl>

</dd></dl>

</section>
<section id="chat-message">
<h2>Chat Message<a class="headerlink" href="#chat-message" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="nano_llm.ChatMessage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">ChatMessage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">role</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'user'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/chat/message.html#ChatMessage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.ChatMessage" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Create a chat entry consisting of a text message, image, ect as input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>role</strong> (<em>str</em>) – The chat’s turn template to apply, typically ‘user’ or ‘bot’.
The role should have a corresponding entry in the active ChatTemplate.</p></li>
<li><p><strong>text</strong> (<em>str</em>) – String containing the message’s content for text messages.</p></li>
<li><p><strong>image</strong> (<em>str</em><em>|</em><em>image</em>) – Either a np.ndarray, torch.Tensor, cudaImage, PIL.Image,
or a path to an image file (.jpg, .png, .bmp, ect)</p></li>
<li><p><strong>kwargs</strong> – <p>For messages with alternate content types, pass them in via kwargs
and they will automatically be determined like so:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">message</span> <span class="o">=</span> <span class="n">ChatMessage</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="n">audio</span><span class="o">=</span><span class="s1">&#39;sounds.wav&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>There are additional lower-level kwargs that can be set below.</p>
</p></li>
<li><p><strong>use_cache</strong> (<em>bool</em>) – cache the tokens/embeddings for reused prompts (defaults to false)</p></li>
<li><p><strong>tokens</strong> (<em>list</em><em>[</em><em>int</em><em>] or </em><em>np.ndarray</em>) – the message contents already having been tokenized</p></li>
<li><p><strong>embedding</strong> (<em>np.ndarray</em>) – the message contents already having been embedded</p></li>
<li><p><strong>history</strong> (<a class="reference internal" href="#nano_llm.ChatHistory" title="nano_llm.ChatHistory"><em>ChatHistory</em></a>) – the ChatHistory object this message belongs to</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="nano_llm.ChatMessage.role">
<span class="sig-name descname"><span class="pre">role</span></span><a class="headerlink" href="#nano_llm.ChatMessage.role" title="Link to this definition"></a></dt>
<dd><p>The user role or character (‘user’, ‘assistant’, ‘system’, ect)</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nano_llm.ChatMessage.template">
<span class="sig-name descname"><span class="pre">template</span></span><a class="headerlink" href="#nano_llm.ChatMessage.template" title="Link to this definition"></a></dt>
<dd><p>The version of this message with the role template applied</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nano_llm.ChatMessage.tokens">
<span class="sig-name descname"><span class="pre">tokens</span></span><a class="headerlink" href="#nano_llm.ChatMessage.tokens" title="Link to this definition"></a></dt>
<dd><p>The tokenized version of the message</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nano_llm.ChatMessage.embedding">
<span class="sig-name descname"><span class="pre">embedding</span></span><a class="headerlink" href="#nano_llm.ChatMessage.embedding" title="Link to this definition"></a></dt>
<dd><p>The embedding of the message</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nano_llm.ChatMessage.history">
<span class="sig-name descname"><span class="pre">history</span></span><a class="headerlink" href="#nano_llm.ChatMessage.history" title="Link to this definition"></a></dt>
<dd><p>The ChatHistory object this message belongs to</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nano_llm.ChatMessage.use_cache">
<span class="sig-name descname"><span class="pre">use_cache</span></span><a class="headerlink" href="#nano_llm.ChatMessage.use_cache" title="Link to this definition"></a></dt>
<dd><p>Set to true if the tokens/embeddings should be cached for reused prompts</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nano_llm.ChatMessage.cached">
<span class="sig-name descname"><span class="pre">cached</span></span><a class="headerlink" href="#nano_llm.ChatMessage.cached" title="Link to this definition"></a></dt>
<dd><p>Set to true if the message is already in the chat embedding</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nano_llm.ChatMessage.index">
<span class="sig-name descname"><span class="pre">index</span></span><a class="headerlink" href="#nano_llm.ChatMessage.index" title="Link to this definition"></a></dt>
<dd><p>The index of this message in the chat history</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nano_llm.ChatMessage.prev">
<span class="sig-name descname"><span class="pre">prev</span></span><a class="headerlink" href="#nano_llm.ChatMessage.prev" title="Link to this definition"></a></dt>
<dd><p>The previous message in the chat history</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nano_llm.ChatMessage.next">
<span class="sig-name descname"><span class="pre">next</span></span><a class="headerlink" href="#nano_llm.ChatMessage.next" title="Link to this definition"></a></dt>
<dd><p>The next message in the chat history</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nano_llm.ChatMessage.type">
<span class="sig-name descname"><span class="pre">type</span></span><a class="headerlink" href="#nano_llm.ChatMessage.type" title="Link to this definition"></a></dt>
<dd><p>The type of the message (‘text’, ‘image’, ‘audio’, ect)</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="nano_llm.ChatMessage.content">
<span class="sig-name descname"><span class="pre">content</span></span><a class="headerlink" href="#nano_llm.ChatMessage.content" title="Link to this definition"></a></dt>
<dd><p>The content or media contained in the message</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nano_llm.ChatMessage.num_tokens">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">num_tokens</span></span><a class="headerlink" href="#nano_llm.ChatMessage.num_tokens" title="Link to this definition"></a></dt>
<dd><p>Return the number of tokens used by this message.
embed() needs to have been called for this to be valid.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nano_llm.ChatMessage.start_token">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">start_token</span></span><a class="headerlink" href="#nano_llm.ChatMessage.start_token" title="Link to this definition"></a></dt>
<dd><p>The token offset or position in the chat history at which this message begins.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.ChatMessage.content_type">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">content_type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">content</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/chat/message.html#ChatMessage.content_type"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.ChatMessage.content_type" title="Link to this definition"></a></dt>
<dd><p>Try to automatically determine the message content type.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.ChatMessage.is_type">
<span class="sig-name descname"><span class="pre">is_type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">type</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/chat/message.html#ChatMessage.is_type"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.ChatMessage.is_type" title="Link to this definition"></a></dt>
<dd><p>Return true if the message is of the given type (like ‘text’, ‘image’, ect)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.ChatMessage.embed">
<span class="sig-name descname"><span class="pre">embed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">return_tensors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'np'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/chat/message.html#ChatMessage.embed"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.ChatMessage.embed" title="Link to this definition"></a></dt>
<dd><p>Apply message templates, tokenization, and generate the embedding.</p>
</dd></dl>

</dd></dl>

</section>
<section id="function-calling">
<h2>Function Calling<a class="headerlink" href="#function-calling" title="Link to this heading"></a></h2>
<p>You can expose Python functions that the model is able to invoke using its code generation abilities, should you so instruct it to.  A list of functions can be provided to <a class="reference internal" href="#model-api"><span class="xref myst"><code class="docutils literal notranslate"><span class="pre">NanoLLM.generate()</span></code></span></a> that will be called inline with the generation, and recieve the output produced so far by the model.</p>
<iframe width="720" height="405" src="https://www.youtube.com/embed/7lKBJPpasAQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<br/>
<br/><p>These functions can then parse the text from the bot to determine if it was called, and execute it accordingly.  Any text returned by these functions will be added to the chat before resuming generation, so the bot is able to utilize them the rest of its reply.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">bot_function()</span></code> decorator automatically wraps Python functions, performs regex matching on the model output, runs them if they were called using Python <code class="docutils literal notranslate"><span class="pre">eval()</span></code>, and returns any results:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nano_llm</span> <span class="kn">import</span> <span class="n">NanoLLM</span><span class="p">,</span> <span class="n">ChatHistory</span><span class="p">,</span> <span class="n">BotFunctions</span><span class="p">,</span> <span class="n">bot_function</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="nd">@bot_function</span>
<span class="k">def</span> <span class="nf">DATE</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the current date. &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%A, %B %-m %Y&quot;</span><span class="p">)</span>
   
<span class="nd">@bot_function</span>
<span class="k">def</span> <span class="nf">TIME</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Returns the current time. &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%-I:%M %p&quot;</span><span class="p">)</span>
          
<span class="c1"># load the model   </span>
<span class="n">model</span> <span class="o">=</span> <span class="n">NanoLLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span><span class="p">,</span> 
    <span class="n">quantization</span><span class="o">=</span><span class="s1">&#39;q4f16_ft&#39;</span><span class="p">,</span> 
    <span class="n">api</span><span class="o">=</span><span class="s1">&#39;mlc&#39;</span>
<span class="p">)</span>

<span class="c1"># create the chat history</span>
<span class="n">system_prompt</span> <span class="o">=</span> <span class="s2">&quot;You are a helpful and friendly AI assistant.&quot;</span> <span class="o">+</span> <span class="n">BotFunctions</span><span class="o">.</span><span class="n">generate_docs</span><span class="p">()</span>
<span class="n">chat_history</span> <span class="o">=</span> <span class="n">ChatHistory</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">system_prompt</span><span class="o">=</span><span class="n">system_prompt</span><span class="p">)</span>

<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="c1"># enter the user query from terminal</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&gt;&gt; &#39;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="nb">input</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

    <span class="c1"># add user prompt and generate chat tokens/embeddings</span>
    <span class="n">chat_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="o">=</span><span class="n">prompt</span><span class="p">)</span>
    <span class="n">embedding</span><span class="p">,</span> <span class="n">position</span> <span class="o">=</span> <span class="n">chat_history</span><span class="o">.</span><span class="n">embed_chat</span><span class="p">()</span>

    <span class="c1"># generate bot reply (give it function access)</span>
    <span class="n">reply</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="n">embedding</span><span class="p">,</span> 
        <span class="n">streaming</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
        <span class="n">functions</span><span class="o">=</span><span class="n">BotFunctions</span><span class="p">(),</span>
        <span class="n">kv_cache</span><span class="o">=</span><span class="n">chat_history</span><span class="o">.</span><span class="n">kv_cache</span><span class="p">,</span>
        <span class="n">stop_tokens</span><span class="o">=</span><span class="n">chat_history</span><span class="o">.</span><span class="n">template</span><span class="o">.</span><span class="n">stop</span>
    <span class="p">)</span>
        
    <span class="c1"># stream the output</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">reply</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span> <span class="k">if</span> <span class="n">reply</span><span class="o">.</span><span class="n">eos</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># save the final output</span>
    <span class="n">chat_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s1">&#39;bot&#39;</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="n">reply</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">tokens</span><span class="o">=</span><span class="n">reply</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>
    <span class="n">chat_history</span><span class="o">.</span><span class="n">kv_cache</span> <span class="o">=</span> <span class="n">reply</span><span class="o">.</span><span class="n">kv_cache</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="nano_llm.bot_function">
<span class="sig-name descname"><span class="pre">bot_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">func</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">docs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'pydoc'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">code</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'python'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enabled</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/plugins/bot_functions.html#bot_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.bot_function" title="Link to this definition"></a></dt>
<dd><p>Decorator for exposing a function to be callable by the LLM.
This will create wrapper functions that do the parsing to
determine if this function was called in the output text,
and then interpret it to invoke the function call.
Text returned from these functions will be added to the chat.</p>
<p>For example, this definition will expose the <code class="docutils literal notranslate"><span class="pre">TIME()</span></code> function to the bot:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@bot_function</span>
<span class="k">def</span> <span class="nf">TIME</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39; Returns the current time. &#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%-I:%M %p&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>You should then add instructions for calling it to the system prompt so that
the bot knows it’s available. <a class="reference internal" href="#nano_llm.BotFunctions.generate_docs" title="nano_llm.BotFunctions.generate_docs"><code class="xref py py-meth docutils literal notranslate"><span class="pre">BotFunctions.generate_docs()</span></code></a> can automatically
generate the function descriptions for you from their Python docstrings, which
you can then add to the chat history.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> (<em>Callable</em>) – The function to be called by the model.</p></li>
<li><p><strong>name</strong> (<em>str</em>) – The function name that the model should refer to.
By default, it will be the actual Python function name.</p></li>
<li><p><strong>docs</strong> (<em>str</em>) – Description of the function that is added to the model’s
system prompt.  By default, the Python docstring is used
from the function’s code comment block (<cite>‘’’ docs here ‘’’</cite>)</p></li>
<li><p><strong>code</strong> (<em>str</em>) – Language that the model is expected to write code in.
By default this is Python, but JSON will be added also.</p></li>
<li><p><strong>enabled</strong> (<em>bool</em>) – Boolean that toggles whether this function is added
to the system prompt and able to be called or not.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nano_llm.BotFunctions">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">BotFunctions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">all</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">load</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/plugins/bot_functions.html#BotFunctions"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.BotFunctions" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Manager of functions able to be called by the LLM that have been registered
with the <a class="reference internal" href="#nano_llm.bot_function" title="nano_llm.bot_function"><code class="xref py py-func docutils literal notranslate"><span class="pre">bot_function()</span></code></a> decorator or <a class="reference internal" href="#nano_llm.BotFunctions.register" title="nano_llm.BotFunctions.register"><code class="xref py py-meth docutils literal notranslate"><span class="pre">BotFunctions.register()</span></code></a>.
This is a singleton that is mostly intended to be used like a list,
where <code class="docutils literal notranslate"><span class="pre">BotFunction()</span></code> returns the currently enabled functions.</p>
<p>You can pass these to <a class="reference internal" href="models.html#nano_llm.NanoLLM.generate" title="nano_llm.NanoLLM.generate"><code class="xref py py-meth docutils literal notranslate"><span class="pre">NanoLLM.generate()</span></code></a>, and they will be called inline
with the generation:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">BotFunctions</span><span class="p">()</span><span class="o">.</span><span class="n">generate_docs</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot;What is the date?&quot;</span><span class="p">,</span>
    <span class="n">functions</span><span class="o">=</span><span class="n">BotFunctions</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
<p><a class="reference internal" href="#nano_llm.BotFunctions.generate_docs" title="nano_llm.BotFunctions.generate_docs"><code class="xref py py-meth docutils literal notranslate"><span class="pre">BotFunctions.generate_docs()</span></code></a> will automatically generate function descriptions
from their Python docstrings.  You can filter and disable functions with <a class="reference internal" href="#nano_llm.BotFunctions.filter" title="nano_llm.BotFunctions.filter"><code class="xref py py-meth docutils literal notranslate"><span class="pre">BotFunctions.filter()</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.BotFunctions.__new__">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">__new__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cls</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">all</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">load</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/plugins/bot_functions.html#BotFunctions.__new__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.BotFunctions.__new__" title="Link to this definition"></a></dt>
<dd><p>Return the list of enabled functions whenever <cite>BotFunctions()</cite> is called,
making it seem like you are just calling a function that returns a list:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">func</span> <span class="ow">in</span> <span class="n">BotFunctions</span><span class="p">():</span>
   <span class="n">func</span><span class="p">(</span><span class="s2">&quot;SQRT(64)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>If <cite>all=True</cite>, then even the disabled functions will be included.
If <cite>load=True</cite>, then the built-in functions will be loaded (if they haven’t yet been).
If <cite>test=True</cite>, then the built-in functions will be tested (if they haven’t yet been).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.BotFunctions.len">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">len</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/plugins/bot_functions.html#BotFunctions.len"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.BotFunctions.len" title="Link to this definition"></a></dt>
<dd><p>Returns the number of all registered bot functions.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.BotFunctions.list">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">all</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/plugins/bot_functions.html#BotFunctions.list"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.BotFunctions.list" title="Link to this definition"></a></dt>
<dd><p>Return the list of all enabled functions available to the bot.
If <cite>all=True</cite>, then even the disabled functions will be included.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.BotFunctions.filter">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">filter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'enable'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/plugins/bot_functions.html#BotFunctions.filter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.BotFunctions.filter" title="Link to this definition"></a></dt>
<dd><p>Apply filters to the registered functions, either enabling or disabling them
if their names are matched against the filter list.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.BotFunctions.generate_docs">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">generate_docs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prologue</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epilogue</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/plugins/bot_functions.html#BotFunctions.generate_docs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.BotFunctions.generate_docs" title="Link to this definition"></a></dt>
<dd><p>Collate the documentation strings from all the enabled functions</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.BotFunctions.register">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">register</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">func</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">docs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'pydoc'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">code</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'python'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enabled</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/plugins/bot_functions.html#BotFunctions.register"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.BotFunctions.register" title="Link to this definition"></a></dt>
<dd><p>See the docs for <a class="reference internal" href="#nano_llm.bot_function" title="nano_llm.bot_function"><code class="xref py py-func docutils literal notranslate"><span class="pre">bot_function()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.BotFunctions.load">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">test</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/plugins/bot_functions.html#BotFunctions.load"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.BotFunctions.load" title="Link to this definition"></a></dt>
<dd><p>Load the built-in functions by importing their modules.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nano_llm.BotFunctions.test">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">test</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">disable_on_error</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/nano_llm/plugins/bot_functions.html#BotFunctions.test"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nano_llm.BotFunctions.test" title="Link to this definition"></a></dt>
<dd><p>Test that the functions are able to be run, and disable them if not.
Returns true if all tests passed, otherwise false.</p>
</dd></dl>

</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="models.html" class="btn btn-neutral float-left" title="Models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="multimodal.html" class="btn btn-neutral float-right" title="Multimodal" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>